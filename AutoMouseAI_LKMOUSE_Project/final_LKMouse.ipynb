{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5.QtWidgets import *\n",
    "from PyQt5.QtCore import *\n",
    "import sys\n",
    "import random\n",
    "import pyautogui\n",
    "from PyQt5 import QtWidgets, uic\n",
    "import icon_rc_\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import csv #엑셀파일\n",
    "import os\n",
    "import win32com.client \n",
    "from openpyxl import Workbook\n",
    "import xlwt\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl \n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [],[] \n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back):]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i:(i + look_back):])\n",
    "    \n",
    "    dataX=np.squeeze(dataX,axis=1) #차원 축소\n",
    "    dataY=np.squeeze(dataY,axis=1) #차원 축소\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1094, 99, 3)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "data1=pd.read_excel('x_data_final.xls') #x좌표 엑셀 데이터 불러옴 \n",
    "data2=pd.read_excel('y_data_final.xls') #y좌표 엑셀 데이터 불러옴 \n",
    "data3=pd.read_excel('speed_final.xls') #speed좌표 엑셀 데이터 불러옴 \n",
    "\n",
    "pddata1=pd.DataFrame(data1) \n",
    "pddata1.head()\n",
    "pddata2=pd.DataFrame(data2) \n",
    "pddata2.head()\n",
    "pddata3=pd.DataFrame(data3) \n",
    "pddata3.head()\n",
    "\n",
    "hap=[] #진짜 합\n",
    "for j in range(len(pddata1)):\n",
    "    x_data=np.array(pddata1.loc[j])\n",
    "    y_data=np.array(pddata2.loc[j])\n",
    "    s_data=np.array(pddata3.loc[j])\n",
    "\n",
    "    hap1=[] #[x,y,속력] 데이터를 저장하고 있는 리스트\n",
    "    \n",
    "    for i in range(len(s_data)): \n",
    "        sum=[] #리스트 하나당 임시로 x,y,속력을 저장할 리스트 (for문 돌릴때마다 초기화)\n",
    "        sum.append(x_data[i]) #x넣음\n",
    "        sum.append(y_data[i]) #y넣음\n",
    "        sum.append(s_data[i]) #속력넣음\n",
    "        hap1.append(sum) #[x,y,속력] 하나의 리스트를 hap리스트에 넣음  \n",
    "\n",
    "    j = j + 1\n",
    "    \n",
    "    hap.append(hap1)\n",
    "\n",
    "n1 = np.array(hap)\n",
    "print(n1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = n1.shape[0] # 257개 Sample 데이터\n",
    "num_sequence = n1.shape[1] # 99개 시계열 데이터\n",
    "num_feature = n1.shape[2] #3개 Feature\n",
    "\n",
    "dataset = n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.70) #학습 데이터 70%\n",
    "test_size = len(dataset) - train_size #테스트 데이터 30%\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 1 #이전 시간 단계 입력변수\n",
    "#X는 지금 t 값이고, Y는 그 다음의 t+1 값임 (즉, X=t, Y=t+1)\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 610 samples, validate on 153 samples\n",
      "Epoch 1/10\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 1433.5840 - accuracy: 0.4458 - val_loss: 1358.2503 - val_accuracy: 0.7516\n",
      "Epoch 2/10\n",
      "610/610 [==============================] - 2s 3ms/step - loss: 1203.7598 - accuracy: 0.9794 - val_loss: 1202.5396 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "610/610 [==============================] - 2s 3ms/step - loss: 1084.8052 - accuracy: 1.0000 - val_loss: 1092.3576 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "610/610 [==============================] - 2s 3ms/step - loss: 988.2468 - accuracy: 1.0000 - val_loss: 1035.2482 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "610/610 [==============================] - 2s 3ms/step - loss: 952.2644 - accuracy: 1.0000 - val_loss: 1011.3145 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 933.0470 - accuracy: 1.0000 - val_loss: 996.0141 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "610/610 [==============================] - 1s 2ms/step - loss: 920.3874 - accuracy: 1.0000 - val_loss: 985.1788 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "610/610 [==============================] - 1s 2ms/step - loss: 911.3170 - accuracy: 1.0000 - val_loss: 977.1686 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "610/610 [==============================] - 2s 2ms/step - loss: 904.5223 - accuracy: 1.0000 - val_loss: 971.0736 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "610/610 [==============================] - 2s 3ms/step - loss: 899.2895 - accuracy: 1.0000 - val_loss: 966.2991 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import *\n",
    "\n",
    "model = Sequential() \n",
    "model.add(LSTM(32,return_sequences=True, input_shape=(num_sequence,num_feature)))  \n",
    "model.add(Dense(3, activation='softmax')) # 3개의 예측\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "\n",
    "history = model.fit(trainX, trainY, epochs=10, batch_size=20, validation_split=0.2, verbose=1)\n",
    "\n",
    "model.save('lstm_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327/327 [==============================] - 0s 733us/step\n",
      "정확도:  [990.2615253786793, 0.9970963597297668]\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도: \",(model.evaluate(testX, testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
