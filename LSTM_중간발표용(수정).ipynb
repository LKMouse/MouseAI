{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# LSTM for international airline passengers problem with memory\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [],[] \n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back):]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i:(i + look_back):])\n",
    "    \n",
    "    dataX=np.squeeze(dataX,axis=1) #차원 축소\n",
    "    dataY=np.squeeze(dataY,axis=1) #차원 축소\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257, 99, 3)\n"
     ]
    }
   ],
   "source": [
    "data1=pd.read_excel('x_data.xls') #x좌표 엑셀 데이터 불러옴 \n",
    "data2=pd.read_excel('y_data.xls') #y좌표 엑셀 데이터 불러옴 \n",
    "data3=pd.read_excel('speed.xls') #speed좌표 엑셀 데이터 불러옴 \n",
    "\n",
    "pddata1=pd.DataFrame(data1) \n",
    "pddata1.head()\n",
    "pddata2=pd.DataFrame(data2) \n",
    "pddata2.head()\n",
    "pddata3=pd.DataFrame(data3) \n",
    "pddata3.head()\n",
    "\n",
    "hap=[] #진짜 합\n",
    "for j in range(len(pddata1)):\n",
    "    x_data=np.array(pddata1.loc[j])\n",
    "    y_data=np.array(pddata2.loc[j])\n",
    "    s_data=np.array(pddata3.loc[j])\n",
    "\n",
    "    hap1=[] #[x,y,속력] 데이터를 저장하고 있는 리스트\n",
    "    \n",
    "    for i in range(len(s_data)): \n",
    "        sum=[] #리스트 하나당 임시로 x,y,속력을 저장할 리스트 (for문 돌릴때마다 초기화)\n",
    "        sum.append(x_data[i]) #x넣음\n",
    "        sum.append(y_data[i]) #y넣음\n",
    "        sum.append(s_data[i]) #속력넣음\n",
    "        hap1.append(sum) #[x,y,속력] 하나의 리스트를 hap리스트에 넣음  \n",
    "\n",
    "    j = j + 1\n",
    "    \n",
    "    hap.append(hap1)\n",
    "\n",
    "n1 = np.array(hap)\n",
    "print(n1.shape) #257,99,3 257개의 Sample과 99개 시계열, 3개 feature(x,y,속력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = n1.shape[0] # 257개 Sample 데이터\n",
    "num_sequence = n1.shape[1] # 99개 시계열 데이터\n",
    "num_feature = n1.shape[2] #3개 Feature\n",
    "\n",
    "dataset = n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[388.        , 252.        ,   0.        ],\n",
       "        [388.        , 252.        ,   0.        ],\n",
       "        [388.        , 252.        ,   0.        ],\n",
       "        ...,\n",
       "        [224.        , 357.        ,   0.        ],\n",
       "        [224.        , 357.        ,   0.        ],\n",
       "        [224.        , 357.        ,   0.        ]],\n",
       "\n",
       "       [[210.        , 371.        ,   0.        ],\n",
       "        [210.        , 371.        ,   0.        ],\n",
       "        [210.        , 371.        ,   0.        ],\n",
       "        ...,\n",
       "        [335.        , 426.        ,   0.        ],\n",
       "        [335.        , 426.        ,   0.        ],\n",
       "        [335.        , 426.        ,   2.        ]],\n",
       "\n",
       "       [[563.        , 328.        ,   0.        ],\n",
       "        [563.        , 328.        ,   0.        ],\n",
       "        [563.        , 328.        ,   0.        ],\n",
       "        ...,\n",
       "        [407.        , 433.        ,   0.        ],\n",
       "        [407.        , 433.        ,   0.        ],\n",
       "        [407.        , 433.        ,   0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[405.        , 275.        ,   0.        ],\n",
       "        [405.        , 275.        ,   0.        ],\n",
       "        [405.        , 275.        ,   0.        ],\n",
       "        ...,\n",
       "        [206.        , 422.        ,   0.        ],\n",
       "        [206.        , 422.        ,   0.        ],\n",
       "        [206.        , 422.        ,   0.        ]],\n",
       "\n",
       "       [[201.        , 423.        ,   0.        ],\n",
       "        [201.        , 423.        ,   0.        ],\n",
       "        [201.        , 423.        ,   0.        ],\n",
       "        ...,\n",
       "        [180.        , 501.        ,   2.        ],\n",
       "        [180.        , 503.        ,   0.        ],\n",
       "        [180.        , 503.        ,   0.        ]],\n",
       "\n",
       "       [[180.        , 545.        ,   0.        ],\n",
       "        [180.        , 545.        ,   0.        ],\n",
       "        [180.        , 545.        ,   0.        ],\n",
       "        ...,\n",
       "        [219.        , 285.        ,   9.05538514],\n",
       "        [218.        , 276.        ,   0.        ],\n",
       "        [218.        , 276.        ,   9.48683298]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.70) #학습 데이터 70%\n",
    "test_size = len(dataset) - train_size #테스트 데이터 30%\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[388.        , 252.        ,   0.        ],\n",
       "        [388.        , 252.        ,   0.        ],\n",
       "        [388.        , 252.        ,   0.        ],\n",
       "        ...,\n",
       "        [224.        , 357.        ,   0.        ],\n",
       "        [224.        , 357.        ,   0.        ],\n",
       "        [224.        , 357.        ,   0.        ]],\n",
       "\n",
       "       [[210.        , 371.        ,   0.        ],\n",
       "        [210.        , 371.        ,   0.        ],\n",
       "        [210.        , 371.        ,   0.        ],\n",
       "        ...,\n",
       "        [335.        , 426.        ,   0.        ],\n",
       "        [335.        , 426.        ,   0.        ],\n",
       "        [335.        , 426.        ,   2.        ]],\n",
       "\n",
       "       [[563.        , 328.        ,   0.        ],\n",
       "        [563.        , 328.        ,   0.        ],\n",
       "        [563.        , 328.        ,   0.        ],\n",
       "        ...,\n",
       "        [407.        , 433.        ,   0.        ],\n",
       "        [407.        , 433.        ,   0.        ],\n",
       "        [407.        , 433.        ,   0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[379.        , 280.        ,   0.        ],\n",
       "        [379.        , 280.        ,   0.        ],\n",
       "        [379.        , 280.        ,   0.        ],\n",
       "        ...,\n",
       "        [196.        , 391.        ,   0.        ],\n",
       "        [196.        , 391.        ,   2.82842712],\n",
       "        [194.        , 393.        ,   0.        ]],\n",
       "\n",
       "       [[189.        , 400.        ,   0.        ],\n",
       "        [189.        , 400.        ,   0.        ],\n",
       "        [189.        , 400.        ,   0.        ],\n",
       "        ...,\n",
       "        [192.        , 497.        ,   0.        ],\n",
       "        [192.        , 497.        ,   2.        ],\n",
       "        [192.        , 499.        ,   0.        ]],\n",
       "\n",
       "       [[192.        , 528.        ,   0.        ],\n",
       "        [192.        , 528.        ,   0.        ],\n",
       "        [192.        , 528.        ,   0.        ],\n",
       "        ...,\n",
       "        [379.        , 469.        ,   0.        ],\n",
       "        [379.        , 469.        ,   0.        ],\n",
       "        [379.        , 469.        ,   0.        ]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[383.        , 461.        ,   0.        ],\n",
       "        [383.        , 461.        ,   0.        ],\n",
       "        [383.        , 461.        ,   0.        ],\n",
       "        ...,\n",
       "        [414.        , 396.        ,   0.        ],\n",
       "        [414.        , 396.        ,   4.24264069],\n",
       "        [417.        , 393.        ,   0.        ]],\n",
       "\n",
       "       [[511.        , 314.        ,   0.        ],\n",
       "        [511.        , 314.        ,   0.        ],\n",
       "        [511.        , 314.        ,   0.        ],\n",
       "        ...,\n",
       "        [531.        , 443.        ,   0.        ],\n",
       "        [531.        , 443.        ,   1.41421356],\n",
       "        [532.        , 444.        ,   0.        ]],\n",
       "\n",
       "       [[534.        , 456.        ,   0.        ],\n",
       "        [534.        , 456.        ,   0.        ],\n",
       "        [534.        , 456.        ,   0.        ],\n",
       "        ...,\n",
       "        [424.        , 532.        ,   0.        ],\n",
       "        [424.        , 532.        ,   0.        ],\n",
       "        [424.        , 532.        ,   2.82842712]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[405.        , 275.        ,   0.        ],\n",
       "        [405.        , 275.        ,   0.        ],\n",
       "        [405.        , 275.        ,   0.        ],\n",
       "        ...,\n",
       "        [206.        , 422.        ,   0.        ],\n",
       "        [206.        , 422.        ,   0.        ],\n",
       "        [206.        , 422.        ,   0.        ]],\n",
       "\n",
       "       [[201.        , 423.        ,   0.        ],\n",
       "        [201.        , 423.        ,   0.        ],\n",
       "        [201.        , 423.        ,   0.        ],\n",
       "        ...,\n",
       "        [180.        , 501.        ,   2.        ],\n",
       "        [180.        , 503.        ,   0.        ],\n",
       "        [180.        , 503.        ,   0.        ]],\n",
       "\n",
       "       [[180.        , 545.        ,   0.        ],\n",
       "        [180.        , 545.        ,   0.        ],\n",
       "        [180.        , 545.        ,   0.        ],\n",
       "        ...,\n",
       "        [219.        , 285.        ,   9.05538514],\n",
       "        [218.        , 276.        ,   0.        ],\n",
       "        [218.        , 276.        ,   9.48683298]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257, 99, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 99, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 99, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 1 #이전 시간 단계 입력변수\n",
    "#X는 지금 t 값이고, Y는 그 다음의 t+1 값임 (즉, X=t, Y=t+1)\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 99, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 99, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 99, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 99, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 141 samples, validate on 36 samples\n",
      "Epoch 1/300\n",
      "141/141 [==============================] - 1s 11ms/step - loss: 773.8336 - accuracy: 0.4525 - val_loss: 713.9768 - val_accuracy: 0.6655\n",
      "Epoch 2/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 715.2728 - accuracy: 0.8056 - val_loss: 680.1949 - val_accuracy: 0.8967\n",
      "Epoch 3/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 681.7444 - accuracy: 0.8457 - val_loss: 649.1027 - val_accuracy: 0.7912\n",
      "Epoch 4/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 650.1717 - accuracy: 0.7756 - val_loss: 623.0181 - val_accuracy: 0.7738\n",
      "Epoch 5/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 625.7724 - accuracy: 0.7897 - val_loss: 603.0546 - val_accuracy: 0.7851\n",
      "Epoch 6/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 604.8979 - accuracy: 0.8043 - val_loss: 584.2035 - val_accuracy: 0.7884\n",
      "Epoch 7/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 589.4682 - accuracy: 0.8234 - val_loss: 571.2535 - val_accuracy: 0.7980\n",
      "Epoch 8/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 579.1884 - accuracy: 0.8848 - val_loss: 563.3215 - val_accuracy: 0.9610\n",
      "Epoch 9/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 572.5549 - accuracy: 0.9773 - val_loss: 559.0078 - val_accuracy: 0.9405\n",
      "Epoch 10/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 568.5154 - accuracy: 0.9560 - val_loss: 555.6401 - val_accuracy: 0.9596\n",
      "Epoch 11/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 564.9045 - accuracy: 0.9617 - val_loss: 552.3896 - val_accuracy: 0.9517\n",
      "Epoch 12/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 561.8411 - accuracy: 0.9531 - val_loss: 550.4238 - val_accuracy: 0.9416\n",
      "Epoch 13/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 559.7353 - accuracy: 0.9621 - val_loss: 549.0544 - val_accuracy: 0.9630\n",
      "Epoch 14/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 558.5471 - accuracy: 0.9764 - val_loss: 548.1032 - val_accuracy: 0.9621\n",
      "Epoch 15/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 557.6276 - accuracy: 0.9755 - val_loss: 547.3815 - val_accuracy: 0.9630\n",
      "Epoch 16/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 556.8884 - accuracy: 0.9766 - val_loss: 546.7625 - val_accuracy: 0.9632\n",
      "Epoch 17/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 556.1718 - accuracy: 0.9727 - val_loss: 546.2524 - val_accuracy: 0.9635\n",
      "Epoch 18/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 555.6503 - accuracy: 0.9746 - val_loss: 545.8485 - val_accuracy: 0.9200\n",
      "Epoch 19/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 555.1983 - accuracy: 0.9700 - val_loss: 545.5269 - val_accuracy: 0.9012\n",
      "Epoch 20/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 554.8351 - accuracy: 0.9633 - val_loss: 545.1491 - val_accuracy: 0.9102\n",
      "Epoch 21/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 554.5098 - accuracy: 0.9649 - val_loss: 544.7930 - val_accuracy: 0.9543\n",
      "Epoch 22/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 554.2558 - accuracy: 0.9696 - val_loss: 544.5134 - val_accuracy: 0.9529\n",
      "Epoch 23/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 554.0643 - accuracy: 0.9628 - val_loss: 544.3738 - val_accuracy: 0.9557\n",
      "Epoch 24/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 553.8816 - accuracy: 0.9670 - val_loss: 544.2300 - val_accuracy: 0.9130\n",
      "Epoch 25/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 553.6290 - accuracy: 0.9602 - val_loss: 543.9284 - val_accuracy: 0.9021\n",
      "Epoch 26/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 553.4071 - accuracy: 0.9545 - val_loss: 543.6698 - val_accuracy: 0.9349\n",
      "Epoch 27/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 553.2065 - accuracy: 0.9614 - val_loss: 543.5329 - val_accuracy: 0.9380\n",
      "Epoch 28/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 553.0146 - accuracy: 0.9691 - val_loss: 543.4906 - val_accuracy: 0.9116\n",
      "Epoch 29/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 552.8685 - accuracy: 0.9621 - val_loss: 543.3040 - val_accuracy: 0.9094\n",
      "Epoch 30/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 552.7115 - accuracy: 0.9591 - val_loss: 543.1245 - val_accuracy: 0.9335\n",
      "Epoch 31/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 552.5523 - accuracy: 0.9579 - val_loss: 542.9752 - val_accuracy: 0.9313\n",
      "Epoch 32/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 552.4001 - accuracy: 0.9606 - val_loss: 542.8403 - val_accuracy: 0.9388\n",
      "Epoch 33/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 552.2670 - accuracy: 0.9675 - val_loss: 542.7267 - val_accuracy: 0.9386\n",
      "Epoch 34/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 552.1392 - accuracy: 0.9633 - val_loss: 542.6094 - val_accuracy: 0.9363\n",
      "Epoch 35/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 552.0540 - accuracy: 0.9590 - val_loss: 542.5511 - val_accuracy: 0.9155\n",
      "Epoch 36/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 551.9617 - accuracy: 0.9712 - val_loss: 542.4538 - val_accuracy: 0.9192\n",
      "Epoch 37/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 551.8386 - accuracy: 0.9635 - val_loss: 542.3285 - val_accuracy: 0.9262\n",
      "Epoch 38/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 551.7788 - accuracy: 0.9438 - val_loss: 542.2177 - val_accuracy: 0.9425\n",
      "Epoch 39/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 551.6700 - accuracy: 0.9721 - val_loss: 542.1211 - val_accuracy: 0.9489\n",
      "Epoch 40/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 551.5726 - accuracy: 0.9693 - val_loss: 542.0206 - val_accuracy: 0.9408\n",
      "Epoch 41/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 551.4955 - accuracy: 0.9562 - val_loss: 541.9577 - val_accuracy: 0.9214\n",
      "Epoch 42/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 551.4092 - accuracy: 0.9704 - val_loss: 541.8460 - val_accuracy: 0.9470\n",
      "Epoch 43/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 551.3466 - accuracy: 0.9667 - val_loss: 541.7720 - val_accuracy: 0.9478\n",
      "Epoch 44/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 551.2840 - accuracy: 0.9727 - val_loss: 541.7143 - val_accuracy: 0.9526\n",
      "Epoch 45/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 551.2235 - accuracy: 0.9650 - val_loss: 541.6628 - val_accuracy: 0.9467\n",
      "Epoch 46/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 551.1715 - accuracy: 0.9707 - val_loss: 541.6256 - val_accuracy: 0.9526\n",
      "Epoch 47/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 551.1263 - accuracy: 0.9760 - val_loss: 541.5794 - val_accuracy: 0.9529\n",
      "Epoch 48/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 551.0747 - accuracy: 0.9685 - val_loss: 541.5428 - val_accuracy: 0.9509\n",
      "Epoch 49/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 551.0423 - accuracy: 0.9717 - val_loss: 541.5024 - val_accuracy: 0.9529\n",
      "Epoch 50/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 551.0055 - accuracy: 0.9596 - val_loss: 541.4585 - val_accuracy: 0.9610\n",
      "Epoch 51/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.9545 - accuracy: 0.9700 - val_loss: 541.4282 - val_accuracy: 0.9551\n",
      "Epoch 52/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.9188 - accuracy: 0.9736 - val_loss: 541.3979 - val_accuracy: 0.9523\n",
      "Epoch 53/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.9116 - accuracy: 0.9638 - val_loss: 541.3736 - val_accuracy: 0.9571\n",
      "Epoch 54/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.8643 - accuracy: 0.9788 - val_loss: 541.3372 - val_accuracy: 0.9576\n",
      "Epoch 55/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.8275 - accuracy: 0.9731 - val_loss: 541.3113 - val_accuracy: 0.9548\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 1s 5ms/step - loss: 550.8113 - accuracy: 0.9760 - val_loss: 541.2978 - val_accuracy: 0.9576\n",
      "Epoch 57/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.7907 - accuracy: 0.9807 - val_loss: 541.2586 - val_accuracy: 0.9512\n",
      "Epoch 58/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.7640 - accuracy: 0.9632 - val_loss: 541.2406 - val_accuracy: 0.9545\n",
      "Epoch 59/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.7341 - accuracy: 0.9787 - val_loss: 541.2121 - val_accuracy: 0.9548\n",
      "Epoch 60/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.6998 - accuracy: 0.9666 - val_loss: 541.1981 - val_accuracy: 0.9523\n",
      "Epoch 61/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.6921 - accuracy: 0.9721 - val_loss: 541.1791 - val_accuracy: 0.9529\n",
      "Epoch 62/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.6693 - accuracy: 0.9652 - val_loss: 541.1674 - val_accuracy: 0.9543\n",
      "Epoch 63/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.6459 - accuracy: 0.9726 - val_loss: 541.1408 - val_accuracy: 0.9526\n",
      "Epoch 64/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.6263 - accuracy: 0.9703 - val_loss: 541.1132 - val_accuracy: 0.9529\n",
      "Epoch 65/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.5954 - accuracy: 0.9716 - val_loss: 541.0901 - val_accuracy: 0.9565\n",
      "Epoch 66/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.5799 - accuracy: 0.9737 - val_loss: 541.0705 - val_accuracy: 0.9568\n",
      "Epoch 67/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.5614 - accuracy: 0.9738 - val_loss: 541.0569 - val_accuracy: 0.9574\n",
      "Epoch 68/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.5519 - accuracy: 0.9777 - val_loss: 541.0419 - val_accuracy: 0.9565\n",
      "Epoch 69/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.5412 - accuracy: 0.9650 - val_loss: 541.0396 - val_accuracy: 0.9602\n",
      "Epoch 70/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.5246 - accuracy: 0.9774 - val_loss: 541.0170 - val_accuracy: 0.9582\n",
      "Epoch 71/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.5043 - accuracy: 0.9713 - val_loss: 541.0014 - val_accuracy: 0.9590\n",
      "Epoch 72/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.4950 - accuracy: 0.9771 - val_loss: 540.9884 - val_accuracy: 0.9588\n",
      "Epoch 73/300\n",
      "141/141 [==============================] - 0s 3ms/step - loss: 550.4767 - accuracy: 0.9717 - val_loss: 540.9730 - val_accuracy: 0.9602\n",
      "Epoch 74/300\n",
      "141/141 [==============================] - 0s 3ms/step - loss: 550.4680 - accuracy: 0.9750 - val_loss: 540.9606 - val_accuracy: 0.9621\n",
      "Epoch 75/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.4608 - accuracy: 0.9800 - val_loss: 540.9486 - val_accuracy: 0.9613\n",
      "Epoch 76/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.4439 - accuracy: 0.9751 - val_loss: 540.9401 - val_accuracy: 0.9621\n",
      "Epoch 77/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.4449 - accuracy: 0.9767 - val_loss: 540.9302 - val_accuracy: 0.9607\n",
      "Epoch 78/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.4323 - accuracy: 0.9762 - val_loss: 540.9156 - val_accuracy: 0.9638\n",
      "Epoch 79/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.4182 - accuracy: 0.9758 - val_loss: 540.9139 - val_accuracy: 0.9630\n",
      "Epoch 80/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.4273 - accuracy: 0.9760 - val_loss: 540.9023 - val_accuracy: 0.9658\n",
      "Epoch 81/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.4041 - accuracy: 0.9799 - val_loss: 540.8779 - val_accuracy: 0.9630\n",
      "Epoch 82/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3951 - accuracy: 0.9806 - val_loss: 540.8923 - val_accuracy: 0.9630\n",
      "Epoch 83/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.4173 - accuracy: 0.9771 - val_loss: 540.8911 - val_accuracy: 0.9677\n",
      "Epoch 84/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.4111 - accuracy: 0.9714 - val_loss: 540.8708 - val_accuracy: 0.9686\n",
      "Epoch 85/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3887 - accuracy: 0.9791 - val_loss: 540.8560 - val_accuracy: 0.9677\n",
      "Epoch 86/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3753 - accuracy: 0.9800 - val_loss: 540.8570 - val_accuracy: 0.9646\n",
      "Epoch 87/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3664 - accuracy: 0.9746 - val_loss: 540.8641 - val_accuracy: 0.9632\n",
      "Epoch 88/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3672 - accuracy: 0.9798 - val_loss: 540.8595 - val_accuracy: 0.9610\n",
      "Epoch 89/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3580 - accuracy: 0.9769 - val_loss: 540.8488 - val_accuracy: 0.9630\n",
      "Epoch 90/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3642 - accuracy: 0.9726 - val_loss: 540.8472 - val_accuracy: 0.9618\n",
      "Epoch 91/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3475 - accuracy: 0.9790 - val_loss: 540.8388 - val_accuracy: 0.9632\n",
      "Epoch 92/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3478 - accuracy: 0.9782 - val_loss: 540.8298 - val_accuracy: 0.9649\n",
      "Epoch 93/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3479 - accuracy: 0.9753 - val_loss: 540.8208 - val_accuracy: 0.9700\n",
      "Epoch 94/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3488 - accuracy: 0.9777 - val_loss: 540.8288 - val_accuracy: 0.9790\n",
      "Epoch 95/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3357 - accuracy: 0.9831 - val_loss: 540.8279 - val_accuracy: 0.9675\n",
      "Epoch 96/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3452 - accuracy: 0.9759 - val_loss: 540.8077 - val_accuracy: 0.9762\n",
      "Epoch 97/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3505 - accuracy: 0.9686 - val_loss: 540.8261 - val_accuracy: 0.9787\n",
      "Epoch 98/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3470 - accuracy: 0.9832 - val_loss: 540.8288 - val_accuracy: 0.9644\n",
      "Epoch 99/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3346 - accuracy: 0.9782 - val_loss: 540.8203 - val_accuracy: 0.9790\n",
      "Epoch 100/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3274 - accuracy: 0.9808 - val_loss: 540.7992 - val_accuracy: 0.9719\n",
      "Epoch 101/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3208 - accuracy: 0.9817 - val_loss: 540.7963 - val_accuracy: 0.9719\n",
      "Epoch 102/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3149 - accuracy: 0.9771 - val_loss: 540.8010 - val_accuracy: 0.9750\n",
      "Epoch 103/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3101 - accuracy: 0.9837 - val_loss: 540.7946 - val_accuracy: 0.9677\n",
      "Epoch 104/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3099 - accuracy: 0.9778 - val_loss: 540.7895 - val_accuracy: 0.9759\n",
      "Epoch 105/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3082 - accuracy: 0.9842 - val_loss: 540.7921 - val_accuracy: 0.9784\n",
      "Epoch 106/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2980 - accuracy: 0.9867 - val_loss: 540.7754 - val_accuracy: 0.9787\n",
      "Epoch 107/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.2981 - accuracy: 0.9845 - val_loss: 540.7692 - val_accuracy: 0.9764\n",
      "Epoch 108/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2907 - accuracy: 0.9850 - val_loss: 540.7661 - val_accuracy: 0.9787\n",
      "Epoch 109/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3010 - accuracy: 0.9866 - val_loss: 540.7632 - val_accuracy: 0.9798\n",
      "Epoch 110/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2867 - accuracy: 0.9832 - val_loss: 540.7607 - val_accuracy: 0.9804\n",
      "Epoch 111/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2837 - accuracy: 0.9867 - val_loss: 540.7551 - val_accuracy: 0.9801\n",
      "Epoch 112/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.2807 - accuracy: 0.9842 - val_loss: 540.7613 - val_accuracy: 0.9798\n",
      "Epoch 113/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.3122 - accuracy: 0.9872 - val_loss: 540.7592 - val_accuracy: 0.9812\n",
      "Epoch 114/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2929 - accuracy: 0.9844 - val_loss: 540.7730 - val_accuracy: 0.9717\n",
      "Epoch 115/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2877 - accuracy: 0.9820 - val_loss: 540.7747 - val_accuracy: 0.9795\n",
      "Epoch 116/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2878 - accuracy: 0.9843 - val_loss: 540.7733 - val_accuracy: 0.9733\n",
      "Epoch 117/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.2811 - accuracy: 0.9859 - val_loss: 540.7488 - val_accuracy: 0.9823\n",
      "Epoch 118/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2759 - accuracy: 0.9876 - val_loss: 540.7394 - val_accuracy: 0.9826\n",
      "Epoch 119/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2681 - accuracy: 0.9888 - val_loss: 540.7386 - val_accuracy: 0.9829\n",
      "Epoch 120/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2648 - accuracy: 0.9880 - val_loss: 540.7482 - val_accuracy: 0.9818\n",
      "Epoch 121/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2794 - accuracy: 0.9844 - val_loss: 540.7370 - val_accuracy: 0.9851\n",
      "Epoch 122/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.2744 - accuracy: 0.9814 - val_loss: 540.7326 - val_accuracy: 0.9823\n",
      "Epoch 123/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2568 - accuracy: 0.9893 - val_loss: 540.7312 - val_accuracy: 0.9812\n",
      "Epoch 124/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2494 - accuracy: 0.9871 - val_loss: 540.7284 - val_accuracy: 0.9837\n",
      "Epoch 125/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2500 - accuracy: 0.9853 - val_loss: 540.7306 - val_accuracy: 0.9815\n",
      "Epoch 126/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2441 - accuracy: 0.9836 - val_loss: 540.7342 - val_accuracy: 0.9778\n",
      "Epoch 127/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.2383 - accuracy: 0.9871 - val_loss: 540.7326 - val_accuracy: 0.9776\n",
      "Epoch 128/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2443 - accuracy: 0.9841 - val_loss: 540.7410 - val_accuracy: 0.9770\n",
      "Epoch 129/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2428 - accuracy: 0.9812 - val_loss: 540.7118 - val_accuracy: 0.9798\n",
      "Epoch 130/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2225 - accuracy: 0.9866 - val_loss: 540.6971 - val_accuracy: 0.9815\n",
      "Epoch 131/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2120 - accuracy: 0.9879 - val_loss: 540.6953 - val_accuracy: 0.9820\n",
      "Epoch 132/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.2045 - accuracy: 0.9897 - val_loss: 540.6941 - val_accuracy: 0.9790\n",
      "Epoch 133/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2014 - accuracy: 0.9880 - val_loss: 540.6961 - val_accuracy: 0.9795\n",
      "Epoch 134/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2038 - accuracy: 0.9845 - val_loss: 540.6894 - val_accuracy: 0.9798\n",
      "Epoch 135/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2074 - accuracy: 0.9837 - val_loss: 540.6927 - val_accuracy: 0.9764\n",
      "Epoch 136/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1973 - accuracy: 0.9884 - val_loss: 540.6898 - val_accuracy: 0.9745\n",
      "Epoch 137/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1948 - accuracy: 0.9872 - val_loss: 540.6956 - val_accuracy: 0.9742\n",
      "Epoch 138/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2017 - accuracy: 0.9863 - val_loss: 540.6899 - val_accuracy: 0.9806\n",
      "Epoch 139/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1934 - accuracy: 0.9859 - val_loss: 540.6856 - val_accuracy: 0.9795\n",
      "Epoch 140/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1915 - accuracy: 0.9883 - val_loss: 540.6915 - val_accuracy: 0.9733\n",
      "Epoch 141/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1960 - accuracy: 0.9817 - val_loss: 540.7090 - val_accuracy: 0.9806\n",
      "Epoch 142/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1953 - accuracy: 0.9845 - val_loss: 540.6833 - val_accuracy: 0.9778\n",
      "Epoch 143/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1858 - accuracy: 0.9880 - val_loss: 540.6817 - val_accuracy: 0.9767\n",
      "Epoch 144/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1844 - accuracy: 0.9867 - val_loss: 540.6841 - val_accuracy: 0.9728\n",
      "Epoch 145/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1871 - accuracy: 0.9851 - val_loss: 540.6778 - val_accuracy: 0.9776\n",
      "Epoch 146/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1854 - accuracy: 0.9843 - val_loss: 540.6779 - val_accuracy: 0.9790\n",
      "Epoch 147/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1913 - accuracy: 0.9836 - val_loss: 540.6771 - val_accuracy: 0.9742\n",
      "Epoch 148/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2042 - accuracy: 0.9749 - val_loss: 540.7148 - val_accuracy: 0.9784\n",
      "Epoch 149/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1975 - accuracy: 0.9807 - val_loss: 540.6797 - val_accuracy: 0.9739\n",
      "Epoch 150/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1866 - accuracy: 0.9887 - val_loss: 540.6727 - val_accuracy: 0.9820\n",
      "Epoch 151/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1782 - accuracy: 0.9886 - val_loss: 540.6819 - val_accuracy: 0.9745\n",
      "Epoch 152/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1767 - accuracy: 0.9845 - val_loss: 540.6755 - val_accuracy: 0.9848\n",
      "Epoch 153/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1777 - accuracy: 0.9874 - val_loss: 540.6744 - val_accuracy: 0.9829\n",
      "Epoch 154/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1750 - accuracy: 0.9874 - val_loss: 540.6685 - val_accuracy: 0.9804\n",
      "Epoch 155/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1729 - accuracy: 0.9886 - val_loss: 540.6800 - val_accuracy: 0.9725\n",
      "Epoch 156/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1902 - accuracy: 0.9824 - val_loss: 540.6668 - val_accuracy: 0.9818\n",
      "Epoch 157/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1747 - accuracy: 0.9848 - val_loss: 540.6647 - val_accuracy: 0.9778\n",
      "Epoch 158/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1858 - accuracy: 0.9799 - val_loss: 540.6666 - val_accuracy: 0.9815\n",
      "Epoch 159/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1745 - accuracy: 0.9835 - val_loss: 540.6751 - val_accuracy: 0.9843\n",
      "Epoch 160/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1783 - accuracy: 0.9885 - val_loss: 540.6715 - val_accuracy: 0.9731\n",
      "Epoch 161/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1759 - accuracy: 0.9843 - val_loss: 540.6651 - val_accuracy: 0.9846\n",
      "Epoch 162/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1753 - accuracy: 0.9870 - val_loss: 540.6675 - val_accuracy: 0.9815\n",
      "Epoch 163/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2032 - accuracy: 0.9888 - val_loss: 540.6685 - val_accuracy: 0.9762\n",
      "Epoch 164/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1771 - accuracy: 0.9832 - val_loss: 540.6668 - val_accuracy: 0.9820\n",
      "Epoch 165/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1765 - accuracy: 0.9838 - val_loss: 540.6916 - val_accuracy: 0.9837\n",
      "Epoch 166/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1829 - accuracy: 0.9837 - val_loss: 540.6601 - val_accuracy: 0.9798\n",
      "Epoch 167/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1696 - accuracy: 0.9855 - val_loss: 540.6675 - val_accuracy: 0.9843\n",
      "Epoch 168/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1729 - accuracy: 0.9857 - val_loss: 540.6671 - val_accuracy: 0.9820\n",
      "Epoch 169/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1795 - accuracy: 0.9830 - val_loss: 540.6694 - val_accuracy: 0.9736\n",
      "Epoch 170/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1817 - accuracy: 0.9866 - val_loss: 540.6566 - val_accuracy: 0.9801\n",
      "Epoch 171/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1673 - accuracy: 0.9871 - val_loss: 540.6636 - val_accuracy: 0.9820\n",
      "Epoch 172/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1687 - accuracy: 0.9870 - val_loss: 540.6597 - val_accuracy: 0.9806\n",
      "Epoch 173/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1661 - accuracy: 0.9863 - val_loss: 540.6613 - val_accuracy: 0.9764\n",
      "Epoch 174/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1606 - accuracy: 0.9859 - val_loss: 540.6577 - val_accuracy: 0.9806\n",
      "Epoch 175/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1551 - accuracy: 0.9867 - val_loss: 540.6626 - val_accuracy: 0.9818\n",
      "Epoch 176/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1555 - accuracy: 0.9867 - val_loss: 540.6694 - val_accuracy: 0.9840\n",
      "Epoch 177/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1659 - accuracy: 0.9827 - val_loss: 540.7131 - val_accuracy: 0.9719\n",
      "Epoch 178/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2636 - accuracy: 0.9591 - val_loss: 540.6760 - val_accuracy: 0.9806\n",
      "Epoch 179/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2603 - accuracy: 0.9744 - val_loss: 540.7076 - val_accuracy: 0.9804\n",
      "Epoch 180/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.2437 - accuracy: 0.9679 - val_loss: 540.7719 - val_accuracy: 0.9543\n",
      "Epoch 181/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2413 - accuracy: 0.9716 - val_loss: 540.6789 - val_accuracy: 0.9832\n",
      "Epoch 182/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.2142 - accuracy: 0.9812 - val_loss: 540.6686 - val_accuracy: 0.9829\n",
      "Epoch 183/300\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 550.2024 - accuracy: 0.9780 - val_loss: 540.7370 - val_accuracy: 0.9641\n",
      "Epoch 184/300\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 550.1831 - accuracy: 0.9738 - val_loss: 540.6531 - val_accuracy: 0.9806\n",
      "Epoch 185/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1639 - accuracy: 0.9860 - val_loss: 540.6513 - val_accuracy: 0.9832\n",
      "Epoch 186/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1576 - accuracy: 0.9858 - val_loss: 540.6583 - val_accuracy: 0.9767\n",
      "Epoch 187/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1615 - accuracy: 0.9831 - val_loss: 540.6502 - val_accuracy: 0.9837\n",
      "Epoch 188/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1576 - accuracy: 0.9827 - val_loss: 540.6940 - val_accuracy: 0.9787\n",
      "Epoch 189/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1679 - accuracy: 0.9853 - val_loss: 540.6446 - val_accuracy: 0.9823\n",
      "Epoch 190/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1532 - accuracy: 0.9877 - val_loss: 540.6483 - val_accuracy: 0.9823\n",
      "Epoch 191/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1555 - accuracy: 0.9885 - val_loss: 540.6475 - val_accuracy: 0.9812\n",
      "Epoch 192/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1565 - accuracy: 0.9809 - val_loss: 540.6562 - val_accuracy: 0.9829\n",
      "Epoch 193/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1511 - accuracy: 0.9875 - val_loss: 540.6482 - val_accuracy: 0.9832\n",
      "Epoch 194/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1505 - accuracy: 0.9861 - val_loss: 540.6860 - val_accuracy: 0.9742\n",
      "Epoch 195/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1739 - accuracy: 0.9827 - val_loss: 540.6500 - val_accuracy: 0.9826\n",
      "Epoch 196/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1537 - accuracy: 0.9875 - val_loss: 540.6459 - val_accuracy: 0.9837\n",
      "Epoch 197/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1476 - accuracy: 0.9873 - val_loss: 540.6431 - val_accuracy: 0.9823\n",
      "Epoch 198/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1451 - accuracy: 0.9875 - val_loss: 540.6429 - val_accuracy: 0.9846\n",
      "Epoch 199/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1433 - accuracy: 0.9881 - val_loss: 540.6476 - val_accuracy: 0.9843\n",
      "Epoch 200/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1447 - accuracy: 0.9862 - val_loss: 540.6442 - val_accuracy: 0.9840\n",
      "Epoch 201/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1442 - accuracy: 0.9869 - val_loss: 540.6555 - val_accuracy: 0.9820\n",
      "Epoch 202/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1439 - accuracy: 0.9872 - val_loss: 540.6431 - val_accuracy: 0.9834\n",
      "Epoch 203/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1434 - accuracy: 0.9859 - val_loss: 540.6620 - val_accuracy: 0.9823\n",
      "Epoch 204/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1449 - accuracy: 0.9884 - val_loss: 540.6444 - val_accuracy: 0.9829\n",
      "Epoch 205/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1396 - accuracy: 0.9870 - val_loss: 540.6474 - val_accuracy: 0.9820\n",
      "Epoch 206/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1534 - accuracy: 0.9845 - val_loss: 540.6513 - val_accuracy: 0.9820\n",
      "Epoch 207/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1453 - accuracy: 0.9867 - val_loss: 540.6548 - val_accuracy: 0.9815\n",
      "Epoch 208/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1443 - accuracy: 0.9865 - val_loss: 540.6463 - val_accuracy: 0.9834\n",
      "Epoch 209/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1456 - accuracy: 0.9849 - val_loss: 540.6451 - val_accuracy: 0.9815\n",
      "Epoch 210/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1443 - accuracy: 0.9861 - val_loss: 540.6450 - val_accuracy: 0.9829\n",
      "Epoch 211/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1517 - accuracy: 0.9817 - val_loss: 540.6636 - val_accuracy: 0.9784\n",
      "Epoch 212/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1508 - accuracy: 0.9857 - val_loss: 540.6568 - val_accuracy: 0.9823\n",
      "Epoch 213/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1466 - accuracy: 0.9844 - val_loss: 540.6515 - val_accuracy: 0.9829\n",
      "Epoch 214/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1515 - accuracy: 0.9802 - val_loss: 540.6799 - val_accuracy: 0.9708\n",
      "Epoch 215/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1590 - accuracy: 0.9855 - val_loss: 540.6454 - val_accuracy: 0.9832\n",
      "Epoch 216/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1504 - accuracy: 0.9855 - val_loss: 540.6463 - val_accuracy: 0.9834\n",
      "Epoch 217/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1420 - accuracy: 0.9857 - val_loss: 540.6652 - val_accuracy: 0.9820\n",
      "Epoch 218/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1511 - accuracy: 0.9870 - val_loss: 540.6468 - val_accuracy: 0.9846\n",
      "Epoch 219/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1497 - accuracy: 0.9847 - val_loss: 540.6386 - val_accuracy: 0.9846\n",
      "Epoch 220/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1555 - accuracy: 0.9865 - val_loss: 540.6428 - val_accuracy: 0.9848\n",
      "Epoch 221/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1438 - accuracy: 0.9875 - val_loss: 540.6494 - val_accuracy: 0.9809\n",
      "Epoch 222/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1529 - accuracy: 0.9824 - val_loss: 540.6461 - val_accuracy: 0.9843\n",
      "Epoch 223/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1493 - accuracy: 0.9863 - val_loss: 540.6640 - val_accuracy: 0.9798\n",
      "Epoch 224/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1430 - accuracy: 0.9862 - val_loss: 540.6515 - val_accuracy: 0.9826\n",
      "Epoch 225/300\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 550.1474 - accuracy: 0.9848 - val_loss: 540.6354 - val_accuracy: 0.9829\n",
      "Epoch 226/300\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 550.1442 - accuracy: 0.9852 - val_loss: 540.6353 - val_accuracy: 0.9832\n",
      "Epoch 227/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1381 - accuracy: 0.9871 - val_loss: 540.6408 - val_accuracy: 0.9812\n",
      "Epoch 228/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1423 - accuracy: 0.9859 - val_loss: 540.6369 - val_accuracy: 0.9834\n",
      "Epoch 229/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1354 - accuracy: 0.9877 - val_loss: 540.6371 - val_accuracy: 0.9834\n",
      "Epoch 230/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1416 - accuracy: 0.9844 - val_loss: 540.6363 - val_accuracy: 0.9837\n",
      "Epoch 231/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1477 - accuracy: 0.9874 - val_loss: 540.6423 - val_accuracy: 0.9834\n",
      "Epoch 232/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1511 - accuracy: 0.9872 - val_loss: 540.6416 - val_accuracy: 0.9820\n",
      "Epoch 233/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1445 - accuracy: 0.9842 - val_loss: 540.6691 - val_accuracy: 0.9787\n",
      "Epoch 234/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1510 - accuracy: 0.9848 - val_loss: 540.6576 - val_accuracy: 0.9837\n",
      "Epoch 235/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1500 - accuracy: 0.9860 - val_loss: 540.6454 - val_accuracy: 0.9843\n",
      "Epoch 236/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1379 - accuracy: 0.9857 - val_loss: 540.6452 - val_accuracy: 0.9823\n",
      "Epoch 237/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1429 - accuracy: 0.9872 - val_loss: 540.6433 - val_accuracy: 0.9857\n",
      "Epoch 238/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1356 - accuracy: 0.9880 - val_loss: 540.6436 - val_accuracy: 0.9826\n",
      "Epoch 239/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1347 - accuracy: 0.9877 - val_loss: 540.6396 - val_accuracy: 0.9818\n",
      "Epoch 240/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1465 - accuracy: 0.9822 - val_loss: 540.6393 - val_accuracy: 0.9851\n",
      "Epoch 241/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1493 - accuracy: 0.9893 - val_loss: 540.6702 - val_accuracy: 0.9837\n",
      "Epoch 242/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1471 - accuracy: 0.9860 - val_loss: 540.6428 - val_accuracy: 0.9848\n",
      "Epoch 243/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1351 - accuracy: 0.9872 - val_loss: 540.6399 - val_accuracy: 0.9851\n",
      "Epoch 244/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1352 - accuracy: 0.9888 - val_loss: 540.6366 - val_accuracy: 0.9854\n",
      "Epoch 245/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1340 - accuracy: 0.9898 - val_loss: 540.6307 - val_accuracy: 0.9860\n",
      "Epoch 246/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1289 - accuracy: 0.9889 - val_loss: 540.6465 - val_accuracy: 0.9854\n",
      "Epoch 247/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1429 - accuracy: 0.9862 - val_loss: 540.6358 - val_accuracy: 0.9865\n",
      "Epoch 248/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1343 - accuracy: 0.9900 - val_loss: 540.6366 - val_accuracy: 0.9832\n",
      "Epoch 249/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1320 - accuracy: 0.9866 - val_loss: 540.6344 - val_accuracy: 0.9871\n",
      "Epoch 250/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1316 - accuracy: 0.9893 - val_loss: 540.6334 - val_accuracy: 0.9863\n",
      "Epoch 251/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1280 - accuracy: 0.9889 - val_loss: 540.6264 - val_accuracy: 0.9874\n",
      "Epoch 252/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1279 - accuracy: 0.9898 - val_loss: 540.6248 - val_accuracy: 0.9879\n",
      "Epoch 253/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1285 - accuracy: 0.9894 - val_loss: 540.6284 - val_accuracy: 0.9879\n",
      "Epoch 254/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1265 - accuracy: 0.9904 - val_loss: 540.6255 - val_accuracy: 0.9868\n",
      "Epoch 255/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1242 - accuracy: 0.9895 - val_loss: 540.6274 - val_accuracy: 0.9868\n",
      "Epoch 256/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1243 - accuracy: 0.9895 - val_loss: 540.6222 - val_accuracy: 0.9877\n",
      "Epoch 257/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1233 - accuracy: 0.9881 - val_loss: 540.6224 - val_accuracy: 0.9885\n",
      "Epoch 258/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1269 - accuracy: 0.9885 - val_loss: 540.6328 - val_accuracy: 0.9863\n",
      "Epoch 259/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1277 - accuracy: 0.9903 - val_loss: 540.6331 - val_accuracy: 0.9868\n",
      "Epoch 260/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1289 - accuracy: 0.9904 - val_loss: 540.6301 - val_accuracy: 0.9863\n",
      "Epoch 261/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1244 - accuracy: 0.9886 - val_loss: 540.6352 - val_accuracy: 0.9846\n",
      "Epoch 262/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1304 - accuracy: 0.9879 - val_loss: 540.6379 - val_accuracy: 0.9860\n",
      "Epoch 263/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1364 - accuracy: 0.9868 - val_loss: 540.6243 - val_accuracy: 0.9871\n",
      "Epoch 264/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1232 - accuracy: 0.9895 - val_loss: 540.6194 - val_accuracy: 0.9885\n",
      "Epoch 265/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1203 - accuracy: 0.9904 - val_loss: 540.6168 - val_accuracy: 0.9891\n",
      "Epoch 266/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1194 - accuracy: 0.9900 - val_loss: 540.6173 - val_accuracy: 0.9888\n",
      "Epoch 267/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1194 - accuracy: 0.9910 - val_loss: 540.6206 - val_accuracy: 0.9871\n",
      "Epoch 268/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1322 - accuracy: 0.9871 - val_loss: 540.6209 - val_accuracy: 0.9879\n",
      "Epoch 269/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1314 - accuracy: 0.9875 - val_loss: 540.6196 - val_accuracy: 0.9871\n",
      "Epoch 270/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1193 - accuracy: 0.9895 - val_loss: 540.6267 - val_accuracy: 0.9871\n",
      "Epoch 271/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1201 - accuracy: 0.9891 - val_loss: 540.6305 - val_accuracy: 0.9843\n",
      "Epoch 272/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1232 - accuracy: 0.9921 - val_loss: 540.6128 - val_accuracy: 0.9910\n",
      "Epoch 273/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1187 - accuracy: 0.9918 - val_loss: 540.6280 - val_accuracy: 0.9795\n",
      "Epoch 274/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1369 - accuracy: 0.9803 - val_loss: 540.6232 - val_accuracy: 0.9885\n",
      "Epoch 275/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1245 - accuracy: 0.9898 - val_loss: 540.6222 - val_accuracy: 0.9877\n",
      "Epoch 276/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1186 - accuracy: 0.9921 - val_loss: 540.6253 - val_accuracy: 0.9902\n",
      "Epoch 277/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1212 - accuracy: 0.9911 - val_loss: 540.6289 - val_accuracy: 0.9846\n",
      "Epoch 278/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1255 - accuracy: 0.9904 - val_loss: 540.6127 - val_accuracy: 0.9905\n",
      "Epoch 279/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1143 - accuracy: 0.9914 - val_loss: 540.6155 - val_accuracy: 0.9891\n",
      "Epoch 280/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1145 - accuracy: 0.9927 - val_loss: 540.6073 - val_accuracy: 0.9927\n",
      "Epoch 281/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1156 - accuracy: 0.9908 - val_loss: 540.6088 - val_accuracy: 0.9910\n",
      "Epoch 282/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1104 - accuracy: 0.9917 - val_loss: 540.6086 - val_accuracy: 0.9902\n",
      "Epoch 283/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1110 - accuracy: 0.9911 - val_loss: 540.6081 - val_accuracy: 0.9888\n",
      "Epoch 284/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1165 - accuracy: 0.9883 - val_loss: 540.6166 - val_accuracy: 0.9913\n",
      "Epoch 285/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1200 - accuracy: 0.9911 - val_loss: 540.6285 - val_accuracy: 0.9843\n",
      "Epoch 286/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1481 - accuracy: 0.9828 - val_loss: 540.6413 - val_accuracy: 0.9756\n",
      "Epoch 287/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1639 - accuracy: 0.9730 - val_loss: 540.6110 - val_accuracy: 0.9885\n",
      "Epoch 288/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1331 - accuracy: 0.9845 - val_loss: 540.6181 - val_accuracy: 0.9907\n",
      "Epoch 289/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1390 - accuracy: 0.9883 - val_loss: 540.6182 - val_accuracy: 0.9877\n",
      "Epoch 290/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1469 - accuracy: 0.9842 - val_loss: 540.6152 - val_accuracy: 0.9899\n",
      "Epoch 291/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1162 - accuracy: 0.9904 - val_loss: 540.6045 - val_accuracy: 0.9896\n",
      "Epoch 292/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1218 - accuracy: 0.9929 - val_loss: 540.6199 - val_accuracy: 0.9891\n",
      "Epoch 293/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1183 - accuracy: 0.9927 - val_loss: 540.6250 - val_accuracy: 0.9829\n",
      "Epoch 294/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1169 - accuracy: 0.9891 - val_loss: 540.6301 - val_accuracy: 0.9857\n",
      "Epoch 295/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1211 - accuracy: 0.9916 - val_loss: 540.6155 - val_accuracy: 0.9879\n",
      "Epoch 296/300\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 550.1118 - accuracy: 0.9906 - val_loss: 540.6047 - val_accuracy: 0.9896\n",
      "Epoch 297/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1120 - accuracy: 0.9892 - val_loss: 540.6061 - val_accuracy: 0.9896\n",
      "Epoch 298/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1134 - accuracy: 0.9916 - val_loss: 540.6117 - val_accuracy: 0.9837\n",
      "Epoch 299/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1223 - accuracy: 0.9842 - val_loss: 540.6259 - val_accuracy: 0.9848\n",
      "Epoch 300/300\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 550.1336 - accuracy: 0.9865 - val_loss: 540.6053 - val_accuracy: 0.9905\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import *\n",
    "\n",
    "model = Sequential() # Sequeatial Model \n",
    "model.add(LSTM(32,return_sequences=True, input_shape=(num_sequence,num_feature)))  \n",
    "model.add(Dense(3, activation='softmax')) # 3개의 예측\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "\n",
    "history = model.fit(trainX, trainY, epochs=300, batch_size=20, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 1ms/step\n",
      "정확도:  [550.2738422594572, 0.9827219843864441]\n",
      "y:  [[[383.         461.           0.        ]\n",
      "  [383.         461.           0.        ]\n",
      "  [383.         461.           0.        ]\n",
      "  ...\n",
      "  [414.         396.           0.        ]\n",
      "  [414.         396.           4.24264069]\n",
      "  [417.         393.           0.        ]]\n",
      "\n",
      " [[511.         314.           0.        ]\n",
      "  [511.         314.           0.        ]\n",
      "  [511.         314.           0.        ]\n",
      "  ...\n",
      "  [531.         443.           0.        ]\n",
      "  [531.         443.           1.41421356]\n",
      "  [532.         444.           0.        ]]\n",
      "\n",
      " [[534.         456.           0.        ]\n",
      "  [534.         456.           0.        ]\n",
      "  [534.         456.           0.        ]\n",
      "  ...\n",
      "  [424.         532.           0.        ]\n",
      "  [424.         532.           0.        ]\n",
      "  [424.         532.           2.82842712]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[514.         637.           0.        ]\n",
      "  [514.         637.           0.        ]\n",
      "  [514.         637.           0.        ]\n",
      "  ...\n",
      "  [523.         517.           0.        ]\n",
      "  [523.         517.           0.        ]\n",
      "  [523.         517.           0.        ]]\n",
      "\n",
      " [[526.         508.           0.        ]\n",
      "  [526.         508.           0.        ]\n",
      "  [526.         508.           1.        ]\n",
      "  ...\n",
      "  [503.         373.           0.        ]\n",
      "  [503.         373.           6.32455532]\n",
      "  [501.         367.           0.        ]]\n",
      "\n",
      " [[405.         275.           0.        ]\n",
      "  [405.         275.           0.        ]\n",
      "  [405.         275.           0.        ]\n",
      "  ...\n",
      "  [206.         422.           0.        ]\n",
      "  [206.         422.           0.        ]\n",
      "  [206.         422.           0.        ]]] , predict:  [0.60768086 0.38760757 0.00471159 ... 0.33748743 0.6603299  0.00218276]\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도: \",(model.evaluate(testX, testY)))\n",
    "\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "print('y: ',testY,', predict: ',model.predict(trainX).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "plt.plot(epochs, history.history['loss'])\n",
    "plt.plot(epochs, history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257, 99, 2)\n"
     ]
    }
   ],
   "source": [
    "haps=[] #진짜 합\n",
    "for j in range(len(pddata1)):\n",
    "    x_data=np.array(pddata1.loc[j])\n",
    "    y_data=np.array(pddata2.loc[j])\n",
    "\n",
    "    hap2=[] #[x,y,속력] 데이터를 저장하고 있는 리스트\n",
    "    \n",
    "    for i in range(len(s_data)): \n",
    "        sum2=[] #리스트 하나당 임시로 x,y,속력을 저장할 리스트 (for문 돌릴때마다 초기화)\n",
    "        sum2.append(x_data[i]) #x넣음\n",
    "        sum2.append(y_data[i]) #y넣음\n",
    "        hap2.append(sum2) #[x,y,속력] 하나의 리스트를 hap리스트에 넣음  \n",
    "\n",
    "    j = j + 1\n",
    "    \n",
    "    haps.append(hap2)\n",
    "\n",
    "n2 = np.array(haps)\n",
    "print(n2.shape) #257,99,3 257개의 Sample과 99개 시계열, 3개 feature(x,y,속력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample2 = n2.shape[0] # 257개 Sample 데이터\n",
    "num_sequence2 = n2.shape[1] # 99개 시계열 데이터\n",
    "num_feature2 = n2.shape[2] #3개 Feature\n",
    "\n",
    "dataset2 = n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size2 = int(len(dataset2) * 0.70) #학습 데이터 70%\n",
    "test_size2 = len(dataset2) - train_size2 #테스트 데이터 30%\n",
    "train2, test2 = dataset2[0:train_size2,:], dataset2[train_size2:len(dataset2),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[388, 252],\n",
       "        [388, 252],\n",
       "        [388, 252],\n",
       "        ...,\n",
       "        [224, 357],\n",
       "        [224, 357],\n",
       "        [224, 357]],\n",
       "\n",
       "       [[210, 371],\n",
       "        [210, 371],\n",
       "        [210, 371],\n",
       "        ...,\n",
       "        [335, 426],\n",
       "        [335, 426],\n",
       "        [335, 426]],\n",
       "\n",
       "       [[563, 328],\n",
       "        [563, 328],\n",
       "        [563, 328],\n",
       "        ...,\n",
       "        [407, 433],\n",
       "        [407, 433],\n",
       "        [407, 433]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[379, 280],\n",
       "        [379, 280],\n",
       "        [379, 280],\n",
       "        ...,\n",
       "        [196, 391],\n",
       "        [196, 391],\n",
       "        [194, 393]],\n",
       "\n",
       "       [[189, 400],\n",
       "        [189, 400],\n",
       "        [189, 400],\n",
       "        ...,\n",
       "        [192, 497],\n",
       "        [192, 497],\n",
       "        [192, 499]],\n",
       "\n",
       "       [[192, 528],\n",
       "        [192, 528],\n",
       "        [192, 528],\n",
       "        ...,\n",
       "        [379, 469],\n",
       "        [379, 469],\n",
       "        [379, 469]]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[383, 461],\n",
       "        [383, 461],\n",
       "        [383, 461],\n",
       "        ...,\n",
       "        [414, 396],\n",
       "        [414, 396],\n",
       "        [417, 393]],\n",
       "\n",
       "       [[511, 314],\n",
       "        [511, 314],\n",
       "        [511, 314],\n",
       "        ...,\n",
       "        [531, 443],\n",
       "        [531, 443],\n",
       "        [532, 444]],\n",
       "\n",
       "       [[534, 456],\n",
       "        [534, 456],\n",
       "        [534, 456],\n",
       "        ...,\n",
       "        [424, 532],\n",
       "        [424, 532],\n",
       "        [424, 532]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[405, 275],\n",
       "        [405, 275],\n",
       "        [405, 275],\n",
       "        ...,\n",
       "        [206, 422],\n",
       "        [206, 422],\n",
       "        [206, 422]],\n",
       "\n",
       "       [[201, 423],\n",
       "        [201, 423],\n",
       "        [201, 423],\n",
       "        ...,\n",
       "        [180, 501],\n",
       "        [180, 503],\n",
       "        [180, 503]],\n",
       "\n",
       "       [[180, 545],\n",
       "        [180, 545],\n",
       "        [180, 545],\n",
       "        ...,\n",
       "        [219, 285],\n",
       "        [218, 276],\n",
       "        [218, 276]]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 1 #이전 시간 단계 입력변수\n",
    "#X는 지금 t 값이고, Y는 그 다음의 t+1 값임 (즉, X=t, Y=t+1)\n",
    "trainX2, trainY2 = create_dataset(train2, look_back)\n",
    "testX2, testY2 = create_dataset(test2, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[388, 252],\n",
       "        [388, 252],\n",
       "        [388, 252],\n",
       "        ...,\n",
       "        [224, 357],\n",
       "        [224, 357],\n",
       "        [224, 357]],\n",
       "\n",
       "       [[210, 371],\n",
       "        [210, 371],\n",
       "        [210, 371],\n",
       "        ...,\n",
       "        [335, 426],\n",
       "        [335, 426],\n",
       "        [335, 426]],\n",
       "\n",
       "       [[563, 328],\n",
       "        [563, 328],\n",
       "        [563, 328],\n",
       "        ...,\n",
       "        [407, 433],\n",
       "        [407, 433],\n",
       "        [407, 433]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[361, 417],\n",
       "        [361, 417],\n",
       "        [361, 417],\n",
       "        ...,\n",
       "        [311, 368],\n",
       "        [311, 368],\n",
       "        [303, 363]],\n",
       "\n",
       "       [[200, 247],\n",
       "        [200, 247],\n",
       "        [200, 247],\n",
       "        ...,\n",
       "        [278, 256],\n",
       "        [287, 259],\n",
       "        [287, 259]],\n",
       "\n",
       "       [[379, 280],\n",
       "        [379, 280],\n",
       "        [379, 280],\n",
       "        ...,\n",
       "        [196, 391],\n",
       "        [196, 391],\n",
       "        [194, 393]]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y can be no greater than 2-D, but have shapes (177, 99, 2) and (177, 99, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-d9808371c7ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainY2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#초반 부분 train 데이터\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestY2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#후반 부분 test 데이터\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\wonbinai\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2787\u001b[0m     return gca().plot(\n\u001b[0;32m   2788\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2789\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\wonbinai\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m         \"\"\"\n\u001b[0;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1665\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\wonbinai\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\wonbinai\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\wonbinai\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n\u001b[1;32m--> 273\u001b[1;33m                              \"shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y can be no greater than 2-D, but have shapes (177, 99, 2) and (177, 99, 2)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAANQklEQVR4nO3cX2id933H8fdndg3rnzWhUUtnp9QbTlNfNCNR0zDWLV3ZamcXptCLpKVhoWDCmtLLhMHai9ysF4NSktSYYEJv6os1tO5IGwajzSBLFxlSJ05I0VwWay7EaUsHKSw4+e7inE1Cka3H5xxJjr7vFwj0nOcn6asf8tuPj3WeVBWSpO3vd7Z6AEnS5jD4ktSEwZekJgy+JDVh8CWpCYMvSU2sG/wkx5K8nOS5i5xPkm8kWUxyKsmNsx9TkjStIVf4jwAHLnH+ILBv/HYY+Ob0Y0mSZm3d4FfVE8CvLrHkEPCtGnkKuCrJ+2c1oCRpNnbO4HPsBs6uOF4aP/aL1QuTHGb0rwDe8Y533HT99dfP4MtLUh8nT558parmJvnYWQQ/azy25v0aquoocBRgfn6+FhYWZvDlJamPJP856cfO4rd0loBrVxzvAc7N4PNKkmZoFsE/Adw5/m2dW4DfVNWbns6RJG2tdZ/SSfJt4FbgmiRLwFeBtwFU1RHgMeA2YBH4LXDXRg0rSZrcusGvqjvWOV/AF2c2kSRpQ/hKW0lqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpoYFPwkB5K8mGQxyX1rnH93ku8n+WmS00numv2okqRprBv8JDuAB4GDwH7gjiT7Vy37IvB8Vd0A3Ar8Q5JdM55VkjSFIVf4NwOLVXWmql4DjgOHVq0p4F1JArwT+BVwYaaTSpKmMiT4u4GzK46Xxo+t9ADwYeAc8Czw5ap6Y/UnSnI4yUKShfPnz084siRpEkOCnzUeq1XHnwKeAX4f+CPggSS/96YPqjpaVfNVNT83N3fZw0qSJjck+EvAtSuO9zC6kl/pLuDRGlkEfg5cP5sRJUmzMCT4TwP7kuwd/0fs7cCJVWteAj4JkOR9wIeAM7McVJI0nZ3rLaiqC0nuAR4HdgDHqup0krvH548A9wOPJHmW0VNA91bVKxs4tyTpMq0bfICqegx4bNVjR1a8fw74y9mOJkmaJV9pK0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqYlDwkxxI8mKSxST3XWTNrUmeSXI6yY9nO6YkaVo711uQZAfwIPAXwBLwdJITVfX8ijVXAQ8BB6rqpSTv3aiBJUmTGXKFfzOwWFVnquo14DhwaNWazwKPVtVLAFX18mzHlCRNa0jwdwNnVxwvjR9b6Trg6iQ/SnIyyZ1rfaIkh5MsJFk4f/78ZBNLkiYyJPhZ47FadbwTuAn4K+BTwN8lue5NH1R1tKrmq2p+bm7usoeVJE1u3efwGV3RX7vieA9wbo01r1TVq8CrSZ4AbgB+NpMpJUlTG3KF/zSwL8neJLuA24ETq9Z8D/h4kp1J3g58DHhhtqNKkqax7hV+VV1Icg/wOLADOFZVp5PcPT5/pKpeSPJD4BTwBvBwVT23kYNLki5PqlY/Hb855ufna2FhYUu+tiS9VSU5WVXzk3ysr7SVpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpiUHBT3IgyYtJFpPcd4l1H03yepLPzG5ESdIsrBv8JDuAB4GDwH7gjiT7L7Lua8Djsx5SkjS9IVf4NwOLVXWmql4DjgOH1lj3JeA7wMsznE+SNCNDgr8bOLvieGn82P9Lshv4NHDkUp8oyeEkC0kWzp8/f7mzSpKmMCT4WeOxWnX8deDeqnr9Up+oqo5W1XxVzc/NzQ2dUZI0AzsHrFkCrl1xvAc4t2rNPHA8CcA1wG1JLlTVd2cypSRpakOC/zSwL8le4L+A24HPrlxQVXv/7/0kjwD/ZOwl6cqybvCr6kKSexj99s0O4FhVnU5y9/j8JZ+3lyRdGYZc4VNVjwGPrXpszdBX1V9PP5YkadZ8pa0kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqYlBwU9yIMmLSRaT3LfG+c8lOTV+ezLJDbMfVZI0jXWDn2QH8CBwENgP3JFk/6plPwf+rKo+AtwPHJ31oJKk6Qy5wr8ZWKyqM1X1GnAcOLRyQVU9WVW/Hh8+BeyZ7ZiSpGkNCf5u4OyK46XxYxfzBeAHa51IcjjJQpKF8+fPD59SkjS1IcHPGo/VmguTTzAK/r1rna+qo1U1X1Xzc3Nzw6eUJE1t54A1S8C1K473AOdWL0ryEeBh4GBV/XI240mSZmXIFf7TwL4ke5PsAm4HTqxckOQDwKPA56vqZ7MfU5I0rXWv8KvqQpJ7gMeBHcCxqjqd5O7x+SPAV4D3AA8lAbhQVfMbN7Yk6XKlas2n4zfc/Px8LSwsbMnXlqS3qiQnJ72g9pW2ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNTEo+EkOJHkxyWKS+9Y4nyTfGJ8/leTG2Y8qSZrGusFPsgN4EDgI7AfuSLJ/1bKDwL7x22HgmzOeU5I0pSFX+DcDi1V1pqpeA44Dh1atOQR8q0aeAq5K8v4ZzypJmsLOAWt2A2dXHC8BHxuwZjfwi5WLkhxm9C8AgP9J8txlTbt9XQO8stVDXCHci2XuxTL3YtmHJv3AIcHPGo/VBGuoqqPAUYAkC1U1P+Drb3vuxTL3Ypl7scy9WJZkYdKPHfKUzhJw7YrjPcC5CdZIkrbQkOA/DexLsjfJLuB24MSqNSeAO8e/rXML8Juq+sXqTyRJ2jrrPqVTVReS3AM8DuwAjlXV6SR3j88fAR4DbgMWgd8Cdw342kcnnnr7cS+WuRfL3Itl7sWyifciVW96ql2StA35SltJasLgS1ITGx58b8uwbMBefG68B6eSPJnkhq2YczOstxcr1n00yetJPrOZ822mIXuR5NYkzyQ5neTHmz3jZhnwZ+TdSb6f5KfjvRjy/4VvOUmOJXn5Yq9VmribVbVhb4z+k/c/gD8AdgE/BfavWnMb8ANGv8t/C/CTjZxpq94G7sUfA1eP3z/YeS9WrPsXRr8U8JmtnnsLfy6uAp4HPjA+fu9Wz72Fe/G3wNfG788BvwJ2bfXsG7AXfwrcCDx3kfMTdXOjr/C9LcOydfeiqp6sql+PD59i9HqG7WjIzwXAl4DvAC9v5nCbbMhefBZ4tKpeAqiq7bofQ/aigHclCfBORsG/sLljbryqeoLR93YxE3Vzo4N/sVsuXO6a7eByv88vMPobfDtady+S7AY+DRzZxLm2wpCfi+uAq5P8KMnJJHdu2nSba8hePAB8mNELO58FvlxVb2zOeFeUibo55NYK05jZbRm2gcHfZ5JPMAr+n2zoRFtnyF58Hbi3ql4fXcxtW0P2YidwE/BJ4HeBf0vyVFX9bKOH22RD9uJTwDPAnwN/CPxzkn+tqv/e6OGuMBN1c6OD720Zlg36PpN8BHgYOFhVv9yk2TbbkL2YB46PY38NcFuSC1X13c0ZcdMM/TPySlW9Crya5AngBmC7BX/IXtwF/H2NnsheTPJz4Hrg3zdnxCvGRN3c6Kd0vC3DsnX3IskHgEeBz2/Dq7eV1t2LqtpbVR+sqg8C/wj8zTaMPQz7M/I94ONJdiZ5O6O71b6wyXNuhiF78RKjf+mQ5H2M7hx5ZlOnvDJM1M0NvcKvjbstw1vOwL34CvAe4KHxle2F2oZ3CBy4Fy0M2YuqeiHJD4FTwBvAw1W17W4tPvDn4n7gkSTPMnpa496q2na3TU7ybeBW4JokS8BXgbfBdN301gqS1ISvtJWkJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5Ka+F/Xe3Wlc9XddQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainX2,trainY2) #초반 부분 train 데이터\n",
    "plt.show()\n",
    "plt.plot(testX2,testY2) #후반 부분 test 데이터\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
